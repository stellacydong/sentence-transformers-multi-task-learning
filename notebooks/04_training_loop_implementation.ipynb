{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b72759-201a-47e9-bdeb-a73acb05cf03",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (BONUS)\n",
    "If not already done, code the training loop for the Multi-Task Learning Expansion in Task 2.\n",
    "Explain any assumptions or decisions made paying special attention to how training within a\n",
    "MTL framework operates. Please note you need not actually train the model.\n",
    "\n",
    "Things to focus on:\n",
    "- Handling of hypothetical data\n",
    "- Forward pass\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed79637-bb5f-4a1b-a237-bce287335077",
   "metadata": {},
   "source": [
    "In this notebook we implement a multi-task training loop for our `MultiTaskTransformer` from Task 2.  \n",
    "We illustrate:\n",
    "\n",
    "- **Handling of hypothetical data** via `train_loader`  \n",
    "- **Forward pass** through the shared encoder and both task heads  \n",
    "- **Metrics** for each task  \n",
    "\n",
    "> **Note:** This code is illustrative—you don’t need to run it end-to-end. Ensure you have defined:\n",
    "> ```python\n",
    "> model        # MultiTaskTransformer instance\n",
    "> train_loader # yields dicts with input_ids, attention_mask, task_a_labels, task_b_labels\n",
    "> loss_fn_a    # CrossEntropyLoss() for Task A\n",
    "> loss_fn_b    # CrossEntropyLoss() for Task B\n",
    "> optimizer    # e.g. AdamW(model.parameters(), lr=2e-5)\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef583a1f-e6ba-4204-9013-35a64cbd5fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ----- Forward pass -----\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Shared encoder produces:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#   logits_a: shape (B, num_classes_A)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#   logits_b: shape (B, seq_len, num_classes_B)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m logits_a, logits_b \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m---> 25\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,         \u001b[38;5;66;03m# Tensor [B, seq_len]\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;66;03m# Tensor [B, seq_len]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# ----- Task A: Sentence Classification -----\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute cross-entropy loss between logits and true labels\u001b[39;00m\n\u001b[1;32m     31\u001b[0m loss_a \u001b[38;5;241m=\u001b[39m loss_fn_a(logits_a, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_a_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Number of epochs (for illustration)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Set model to training mode (enables dropout, etc.)\n",
    "    model.train()\n",
    "    \n",
    "    # Accumulators for averaged loss and metrics\n",
    "    total_loss = 0.0\n",
    "    correct_a, total_a = 0, 0\n",
    "    correct_b, total_b = 0, 0\n",
    "    \n",
    "    # Iterate over each batch from the DataLoader\n",
    "    for batch in train_loader:\n",
    "        # Reset gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ----- Forward pass -----\n",
    "        # Shared encoder produces:\n",
    "        #   logits_a: shape (B, num_classes_A)\n",
    "        #   logits_b: shape (B, seq_len, num_classes_B)\n",
    "        logits_a, logits_b = model(\n",
    "            input_ids=batch[\"input_ids\"],         # Tensor [B, seq_len]\n",
    "            attention_mask=batch[\"attention_mask\"]# Tensor [B, seq_len]\n",
    "        )\n",
    "        \n",
    "        # ----- Task A: Sentence Classification -----\n",
    "        # Compute cross-entropy loss between logits and true labels\n",
    "        loss_a = loss_fn_a(logits_a, batch[\"task_a_labels\"])\n",
    "        # Derive predicted class indices\n",
    "        preds_a = torch.argmax(logits_a, dim=1)\n",
    "        # Update accuracy counters\n",
    "        correct_a += (preds_a == batch[\"task_a_labels\"]).sum().item()\n",
    "        total_a   += batch[\"task_a_labels\"].size(0)\n",
    "        \n",
    "        # ----- Task B: Named Entity Recognition -----\n",
    "        # logits_b is [B, seq_len, C], flatten for loss\n",
    "        B, L, C = logits_b.shape\n",
    "        loss_b = loss_fn_b(\n",
    "            logits_b.view(-1, C),                # shape (B*L, C)\n",
    "            batch[\"task_b_labels\"].view(-1)       # shape (B*L,)\n",
    "        )\n",
    "        # Compute token-level predictions\n",
    "        preds_b = torch.argmax(logits_b, dim=2)  # shape (B, seq_len)\n",
    "        # Create mask for non-padding tokens (label != -100)\n",
    "        mask = batch[\"task_b_labels\"] != -100    # shape (B, seq_len)\n",
    "        # Update token-level accuracy\n",
    "        correct_b += ((preds_b == batch[\"task_b_labels\"]) & mask).sum().item()\n",
    "        total_b   += mask.sum().item()\n",
    "        \n",
    "        # ----- Combine & Backpropagate -----\n",
    "        # Equal-weight sum of both task losses\n",
    "        loss = loss_a + loss_b\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # ----- End-of-Epoch Metrics -----\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc_a    = correct_a / total_a if total_a else 0.0\n",
    "    acc_b    = correct_b / total_b if total_b else 0.0\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs}  \"\n",
    "        f\"| Loss: {avg_loss:.4f}  \"\n",
    "        f\"| Task A Acc: {acc_a:.4f}  \"\n",
    "        f\"| Task B Token Acc: {acc_b:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417cd4e0-757c-444f-9b64-27f80d783d8a",
   "metadata": {},
   "source": [
    "### Assumptions & Decisions\n",
    "\n",
    "1. **Data Handling**  \n",
    "   - We assume `train_loader` uses our custom `collate_fn`, so each `batch` is a **dict** of tensors:\n",
    "     - `input_ids`, `attention_mask`: shape `(B, seq_len)`\n",
    "     - `task_a_labels`: shape `(B,)`\n",
    "     - `task_b_labels`: shape `(B, seq_len)` with `-100` for padding positions.\n",
    "\n",
    "2. **Forward Pass**  \n",
    "   - A single call to `model(...)` yields two sets of logits:\n",
    "     - **Sentence logits** for Task A\n",
    "     - **Token logits** for Task B\n",
    "\n",
    "3. **Loss Computation**  \n",
    "   - **Task A:** Standard `CrossEntropyLoss` on `(B, num_classes_A)` vs. `(B,)`.  \n",
    "   - **Task B:** Flatten `(B, seq_len, num_classes_B)` and `(B, seq_len)` to apply `CrossEntropyLoss`, ignoring `-100` tokens.\n",
    "\n",
    "4. **Metrics**  \n",
    "   - **Task A Accuracy:** Number of correct sentence predictions divided by total sentences.  \n",
    "   - **Task B Token Accuracy:** Number of correct token predictions (excluding padding) divided by total valid tokens.\n",
    "\n",
    "5. **Loss Aggregation**  \n",
    "   - We sum `loss_a + loss_b` equally.  \n",
    "   - For imbalanced tasks, introduce a weight α:  \n",
    "     ```python\n",
    "     loss = α * loss_a + (1 - α) * loss_b\n",
    "     ```\n",
    "\n",
    "6. **MTL Dynamics**  \n",
    "   - Gradients from both losses flow back through the shared encoder, encouraging representations beneficial to both tasks.\n",
    "   - This joint optimization can improve generalization via inductive transfer between related tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925d737-122e-4a12-bd20-40ad1977f0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6177e8-a9a8-4151-95ae-f706024e32aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9839d6-47b5-4769-829c-7745df977e78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Recreate train_loader with collate_fn so each batch is a dict of tensors\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     10\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn   \u001b[38;5;66;03m# <-- ensures batch[\"input_ids\"], etc. are tensors\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# This means `batch` was coming in as a **list** of examples instead of a **dict** of tensors.  \n",
    "# To fix this, we must pass our custom `collate_fn` when creating the DataLoader:\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Recreate train_loader with collate_fn so each batch is a dict of tensors\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn   # <-- ensures batch[\"input_ids\"], etc. are tensors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d227c-d727-4dcd-ab4c-230efb79e307",
   "metadata": {},
   "source": [
    "### Assumptions & Decisions\n",
    "\n",
    "- **Hypothetical Data**: We rely on our `train_loader` to supply properly tokenized inputs and padded labels.  \n",
    "- **Loss Aggregation**: We sum Task A and Task B losses equally; this can be adjusted via weighting if tasks differ in scale.  \n",
    "- **Metrics**:  \n",
    "  - **Task A**: Sentence‐level accuracy.  \n",
    "  - **Task B**: Token‐level accuracy, excluding padding tokens (label = `-100`).  \n",
    "- **Modularity**: This loop can be extended to include learning‐rate schedulers, gradient clipping, or dynamic loss weighting without altering the core structure.  \n",
    "- **No Actual Training**: This code is illustrative—define real loss functions and optimizer to run on your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339a6f2-bef2-4cc5-94b6-893c579d6103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
