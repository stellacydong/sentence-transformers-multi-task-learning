{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f47f5d-378f-443f-a87a-1305347953b7",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation\n",
    "\n",
    "Implement a sentence transformer model using any deep learning framework of your choice.\n",
    "This model should be able to encode input sentences into fixed-length embeddings. Test your\n",
    "implementation with a few sample sentences and showcase the obtained embeddings.\n",
    "Describe any choices you had to make regarding the model architecture outside of the\n",
    "transformer backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc40c1-a1da-4a16-b007-42f8f4337235",
   "metadata": {},
   "source": [
    "# 1. Import Libraries\n",
    "\n",
    "We import PyTorch and the Hugging Face `transformers` package, as well as NumPy for optional post-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66ec242-5e35-4d08-b7b7-1ae2276434dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pi/miniconda3/envs/cleanenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1d557-e5c3-4cf3-83dd-978a945ba119",
   "metadata": {},
   "source": [
    "2. Define the Sentence Transformer Model\n",
    "\n",
    "We wrap the pretrained transformer in a simple `nn.Module` and add two pooling strategies:\n",
    "- **CLS pooling**: Use the [CLS] token embedding.\n",
    "- **Mean pooling**: Average token embeddings (excluding padding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d167af9f-46d4-4628-a2de-e130b11bd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "class SentenceTransformerModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Sentence Transformer that wraps a pretrained transformer and \n",
    "    applies a pooling strategy to convert token-level embeddings into fixed-length \n",
    "    sentence embeddings.\n",
    "\n",
    "    Supported pooling strategies:\n",
    "      - 'cls': use the [CLS] token embedding.\n",
    "      - 'mean': compute the mean of token embeddings, ignoring padding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', pooling='cls'):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the pretrained model to load from HuggingFace.\n",
    "            pooling (str): Pooling strategy to use. Either 'cls' or 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load the pretrained transformer (e.g., BERT)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        # Store the pooling method (used later in forward pass)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Encodes inputs and reduces token embeddings\n",
    "        to a single sentence-level embedding.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Input token IDs, shape (batch_size, seq_len)\n",
    "            attention_mask (Tensor): Attention mask, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Sentence embeddings, shape (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # Step 1: Pass input through the transformer encoder\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # shape: (B, L, H)\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            # Step 2A: Use the embedding of the [CLS] token (first position)\n",
    "            return last_hidden[:, 0]  # shape: (B, H)\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            # Step 2B: Compute the mean of all token embeddings\n",
    "            # Exclude padding tokens using attention_mask\n",
    "            # Expand mask to match hidden size: (B, L) → (B, L, H)\n",
    "            mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "            # Apply mask and sum across sequence length\n",
    "            summed = torch.sum(last_hidden * mask, dim=1)  # shape: (B, H)\n",
    "            # Count non-padding tokens for each sentence\n",
    "            counts = torch.clamp(mask.sum(dim=1), min=1e-9)  # shape: (B, H)\n",
    "            # Divide sum by count to get mean\n",
    "            return summed / counts  # shape: (B, H)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pooling type. Choose 'cls' or 'mean'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec6e07-9fd1-41ad-b050-5b4381de804b",
   "metadata": {},
   "source": [
    "# 3. Initialize Tokenizer and Model\n",
    "\n",
    "We choose `bert-base-uncased` and default to `cls` pooling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b638498b-eaeb-4581-926d-337e2f500b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pi/miniconda3/envs/cleanenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "pooling_strategy = 'cls'  # or 'mean'\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = SentenceTransformerModel(model_name=model_name, pooling=pooling_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b07d8b-c3f0-4e05-8ca4-15be40ff7338",
   "metadata": {},
   "source": [
    "# 4. Prepare Sample Sentences\n",
    "\n",
    "We’ll encode these three example sentences and inspect their embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd1e5daa-b624-47d6-94f1-645c46a912a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"I love machine learning!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fcf57-14b7-4e78-9641-70e4d1b58133",
   "metadata": {},
   "source": [
    "# 5. Tokenize Sentences\n",
    "\n",
    "The tokenizer handles padding and truncation so that all inputs share the same shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8637fd9a-5bb4-49eb-8d2a-ca20bd9861ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(\n",
    "    sentences,\n",
    "    padding=True,        # Pad to longest in batch\n",
    "    truncation=True,     # Truncate to model’s max length\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5be789-0366-4e1c-ad07-ef18bdb0bdfa",
   "metadata": {},
   "source": [
    "# 6. Generate Embeddings\n",
    "\n",
    "We run a forward pass under `torch.no_grad()` to disable gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2da5a1f4-22ec-4ea7-a956-646a4140656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(\n",
    "        input_ids=encoded['input_ids'],\n",
    "        attention_mask=encoded['attention_mask']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5228a-8ec7-4483-bb57-41e163fc9663",
   "metadata": {},
   "source": [
    "# 7. Inspect the Embeddings\n",
    "\n",
    "We print the raw tensor and its shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8097ca6-ecfa-4048-98bb-3a4228f7959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-length sentence embeddings (one per sentence):\n",
      "tensor([[-0.3642, -0.0531, -0.3673,  ..., -0.3797,  0.5818,  0.4386],\n",
      "        [-0.0397,  0.1847,  0.0180,  ..., -0.5428,  0.3945,  0.3015],\n",
      "        [ 0.2184,  0.2637,  0.0406,  ..., -0.2125,  0.1841,  0.3618]])\n",
      "\n",
      "Embeddings shape: torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Fixed-length sentence embeddings (one per sentence):\")\n",
    "print(embeddings)\n",
    "print(\"\\nEmbeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c8531-de25-48dc-a52a-3db918881677",
   "metadata": {},
   "source": [
    "# 8. Convert to NumPy & Show Sample Values\n",
    "\n",
    "For downstream tasks or visualization, you might convert to NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced9a0f1-d484-4471-bc6a-608aee82f11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \"The cat sat on the mat.\"\n",
      "Embedding (first 8 values): [-0.36422354 -0.05305347 -0.36732256 -0.02967383 -0.46078447 -0.1010612\n",
      "  0.01669887  0.5957765 ] ...\n",
      "\n",
      "Sentence: \"Artificial intelligence is transforming the world.\"\n",
      "Embedding (first 8 values): [-0.03967224  0.18470013  0.01796044 -0.06997652 -0.3907642  -0.6572221\n",
      "  0.8504888   1.0212841 ] ...\n",
      "\n",
      "Sentence: \"I love machine learning!\"\n",
      "Embedding (first 8 values): [ 0.2184243   0.26371536  0.04059215 -0.11266454 -0.32512203 -0.5171517\n",
      "  0.30736122  0.7386393 ] ...\n"
     ]
    }
   ],
   "source": [
    "embeddings_np = embeddings.cpu().numpy()\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence: \\\"{sentence}\\\"\")\n",
    "    # Display first 8 dimensions as a quick sanity check\n",
    "    print(f\"Embedding (first 8 values): {embeddings_np[idx][:8]} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5906c-25a3-47af-a4f0-d69c5dc2c29b",
   "metadata": {},
   "source": [
    "# Design Choices Explanation\n",
    "\n",
    "1. **Transformer Backbone**  \n",
    "   We chose `bert-base-uncased` for its reliable performance and easy accessibility.\n",
    "\n",
    "2. **Pooling Strategy**  \n",
    "   - **CLS pooling** (default) uses the dedicated [CLS] token embedding.  \n",
    "   - **Mean pooling** averages non-padded token embeddings for potentially smoother representations.\n",
    "\n",
    "3. **Tokenization**  \n",
    "   The Hugging Face tokenizer ensures correct token IDs, padding, and truncation in line with the pretrained model’s expectations.\n",
    "\n",
    "4. **Architectural Simplicity**  \n",
    "   No additional projection or normalization layers are added, preserving raw model outputs for pure embedding extraction. Adaptation layers could be added later if needed.\n",
    "\n",
    "5. **Batching & Padding**  \n",
    "   Built-in support for batching makes it easy to process multiple sentences at once without manual padding logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e799c2b-51ae-4365-b8ae-6368d7d66189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
