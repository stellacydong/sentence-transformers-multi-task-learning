{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b72759-201a-47e9-bdeb-a73acb05cf03",
   "metadata": {},
   "source": [
    "# Task 4: Training Loop Implementation (BONUS)\n",
    "If not already done, code the training loop for the Multi-Task Learning Expansion in Task 2.\n",
    "Explain any assumptions or decisions made paying special attention to how training within a\n",
    "MTL framework operates. Please note you need not actually train the model.\n",
    "\n",
    "Things to focus on:\n",
    "- Handling of hypothetical data\n",
    "- Forward pass\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7339a6f2-bef2-4cc5-94b6-893c579d6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import BertModel, BertTokenizer, AdamW\n",
    "# from datasets import load_dataset\n",
    "# import numpy as np\n",
    "\n",
    "# # Device: CPU-only\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Load tokenizer and IMDB dataset\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# # Generate synthetic NER labels (4 classes: O, PER, LOC, ORG)\n",
    "# def create_ner_labels(text):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     return np.random.randint(0, 4, size=len(tokens)).tolist()\n",
    "\n",
    "# # Tokenization + Label generation\n",
    "# def preprocess(batch):\n",
    "#     task_a = [1 if lbl == 1 else 0 for lbl in batch[\"label\"]]  # Binary classification\n",
    "#     task_b = [create_ner_labels(text) for text in batch[\"text\"]]  # Random NER tags\n",
    "#     enc = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "#     return {\n",
    "#         \"input_ids\": enc[\"input_ids\"],\n",
    "#         \"attention_mask\": enc[\"attention_mask\"],\n",
    "#         \"task_a_labels\": task_a,\n",
    "#         \"task_b_labels\": task_b\n",
    "#     }\n",
    "\n",
    "# # Apply preprocessing to dataset\n",
    "# dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "\n",
    "# # Multi-Task Model (shared BERT + two task heads)\n",
    "# class MultiTaskTransformer(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, 2)  # Task A: Sentiment\n",
    "#         self.ner = nn.Linear(self.bert.config.hidden_size, 4)         # Task B: NER\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled = self.dropout(outputs.pooler_output)  # [CLS] for sentence classification\n",
    "#         sequence = self.dropout(outputs.last_hidden_state)  # full sequence for token-level task\n",
    "#         return self.classifier(pooled), self.ner(sequence)\n",
    "\n",
    "# # Instantiate model\n",
    "# model = MultiTaskTransformer().to(device)\n",
    "\n",
    "# # Losses and optimizer\n",
    "# loss_fn_a = nn.CrossEntropyLoss()\n",
    "# loss_fn_b = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Custom collate function to pad token-level labels and stack tensors\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = torch.stack([item[\"input_ids\"] for item in batch]).to(device)\n",
    "#     attention_mask = torch.stack([item[\"attention_mask\"] for item in batch]).to(device)\n",
    "#     task_a = torch.tensor([item[\"task_a_labels\"] for item in batch], dtype=torch.long).to(device)\n",
    "#     task_b = torch.stack([\n",
    "#         F.pad(torch.tensor(item[\"task_b_labels\"], dtype=torch.long), (0, 128 - len(item[\"task_b_labels\"])), value=-100)\n",
    "#         for item in batch\n",
    "#     ]).to(device)\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"task_a_labels\": task_a,\n",
    "#         \"task_b_labels\": task_b\n",
    "#     }\n",
    "\n",
    "# # Dataloader\n",
    "# train_loader = DataLoader(dataset[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# # -------------------------------\n",
    "# # TRAINING LOOP (CPU version)\n",
    "# # -------------------------------\n",
    "# model.train()  # Set model to training mode\n",
    "\n",
    "# # Initialize metrics for Task A (sentence classification)\n",
    "# total_correct_a = 0  # Count of correct predictions\n",
    "# total_seen_a = 0     # Total number of samples seen\n",
    "\n",
    "# # Iterate through the training DataLoader\n",
    "# for step, batch in enumerate(train_loader):\n",
    "#     optimizer.zero_grad()  # Reset gradients from previous step\n",
    "\n",
    "#     # Forward pass through the model for both tasks\n",
    "#     logits_a, logits_b = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "#     # logits_a: shape (batch_size, 2)     → for sentence classification\n",
    "#     # logits_b: shape (batch_size, seq_len, 4) → for token-level NER\n",
    "\n",
    "#     # Compute loss for Task A (sentence classification)\n",
    "#     loss_a = loss_fn_a(logits_a, batch[\"task_a_labels\"])\n",
    "\n",
    "#     # Compute loss for Task B (NER), flatten for token-level CE loss\n",
    "#     # logits_b.view(-1, 4): shape (batch_size * seq_len, 4)\n",
    "#     # labels.view(-1): shape (batch_size * seq_len,)\n",
    "#     # -100 label values will be ignored (padding)\n",
    "#     loss_b = loss_fn_b(logits_b.view(-1, 4), batch[\"task_b_labels\"].view(-1))\n",
    "\n",
    "#     # Total loss = combined multitask objective\n",
    "#     total_loss = loss_a + loss_b\n",
    "\n",
    "#     # Backward pass: compute gradients\n",
    "#     total_loss.backward()\n",
    "\n",
    "#     # Update parameters using gradients\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # -------------------------\n",
    "#     # Task A: Compute accuracy\n",
    "#     # -------------------------\n",
    "#     preds = torch.argmax(logits_a, dim=1)  # Pick class with highest logit\n",
    "#     correct = (preds == batch[\"task_a_labels\"]).sum().item()  # Count correct preds\n",
    "#     total_correct_a += correct\n",
    "#     total_seen_a += len(batch[\"task_a_labels\"])\n",
    "\n",
    "#     # Periodic logging of training progress\n",
    "#     if step % 20 == 0:\n",
    "#         print(f\"Step {step} | LossA: {loss_a.item():.4f} | LossB: {loss_b.item():.4f} | Total: {total_loss.item():.4f}\")\n",
    "\n",
    "#     # Stop after 50 steps (demo/truncated training)\n",
    "#     if step == 50:\n",
    "#         break\n",
    "\n",
    "# # Compute and report final Task A accuracy\n",
    "# print(f\"\\nTask A (Sentiment) Accuracy: {total_correct_a / total_seen_a:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ee757-62dc-4ee5-9a16-2404b85a537a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Assumptions & Explanation for MTL Framework\n",
    "\n",
    "| Aspect                         | Description                                                                                                                                               |\n",
    "| ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Handling Hypothetical Data** | Task B uses **synthetic NER labels** for simplicity. This simulates a real token classification task using random labels instead of annotated ones.       |\n",
    "| **Forward Pass**               | The BERT encoder is shared. Task A (classification) uses the `[CLS]` pooled output. Task B (NER) uses the full hidden sequence for token-wise prediction. |\n",
    "| **Loss Handling**              | Two `CrossEntropyLoss` functions are used and **summed**. Task B uses `ignore_index=-100` to mask padded tokens.                                          |\n",
    "| **Metrics**                    | Only Task A has accuracy tracked. Task B could be evaluated with token-level precision/recall/F1 (not implemented here due to synthetic labels).          |\n",
    "| **Design Choice**              | Both tasks are trained simultaneously, allowing **shared representation learning** and efficient multitask learning.                                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182d379-7565-4de5-8b81-69977b0a6371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
