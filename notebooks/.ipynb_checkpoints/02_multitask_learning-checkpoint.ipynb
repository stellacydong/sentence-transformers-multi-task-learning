{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ec34a-67e0-476f-83da-630bff5d895e",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "Expand the sentence transformer to handle a multi-task learning setting.\n",
    "1. Task A: Sentence Classification – Classify sentences into predefined classes (you can make these up).\n",
    "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition,\n",
    "Sentiment Analysis, etc.] (you can make the labels up)\n",
    "Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962cad1-0531-4e4b-897e-bd9e520634c0",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to extend a sentence-transformer model (BERT) to perform **two** tasks simultaneously:\n",
    "\n",
    "1. **Task A: Sentence Classification**  \n",
    "2. **Task B: Named Entity Recognition (NER)**\n",
    "\n",
    "We share a single encoder and add two task-specific heads. We then train with a combined loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9cfb5-0796-4d8b-bafe-b1a8881d05a8",
   "metadata": {},
   "source": [
    "# 1. Imports Libraries\n",
    "\n",
    "We load PyTorch, HuggingFace Transformers, and the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd726bca-38bd-444b-aaec-f87524c1d874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pi/miniconda3/envs/cleanenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde42689-242b-4792-9e7c-cc2208f1f55c",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "- **Task A**: Sentiment classification on IMDB (binary: positive vs. negative).  \n",
    "- **Task B**: Synthetic NER labels (4 classes: O, PER, LOC, ORG) for demonstration only.\n",
    "\n",
    "We’ll:\n",
    "1. Load the IMDB dataset.\n",
    "2. Tokenize text to fixed length.\n",
    "3. Create randomm NER labels aligned to token count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3634310-7598-40f4-bfa4-8026af3f5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pi/miniconda3/envs/cleanenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Load the IMDB dataset (train & test splits)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 2.2 Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_ner_labels(text: str):\n",
    "    \"\"\"\n",
    "    Generate synthetic token-level labels.\n",
    "    In a real setting, you’d use human-annotated NER tags.\n",
    "    Returns a list of random ints (0-3) matching the tokenized length.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return np.random.randint(0, 4, size=(len(tokens),)).tolist()\n",
    "\n",
    "def preprocess(batch):\n",
    "    \"\"\"\n",
    "    Tokenize the batch of texts, create Task A & B labels.\n",
    "    Returns PyTorch tensors and raw label lists.\n",
    "    \"\"\"\n",
    "    # Convert IMDB 0/1 labels to our binary classification labels\n",
    "    task_a_labels = [1 if lbl == 1 else 0 for lbl in batch[\"label\"]]\n",
    "    \n",
    "    # Generate synthetic NER labels per example\n",
    "    task_b_labels = [create_ner_labels(txt) for txt in batch[\"text\"]]\n",
    "\n",
    "    # Tokenize to max_length=128\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"task_a_labels\": task_a_labels,\n",
    "        \"task_b_labels\": task_b_labels\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to the train split\n",
    "dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\",\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b557e-ddc9-405d-a68a-1c7a443bbcf1",
   "metadata": {},
   "source": [
    "# 3. Model Architecture\n",
    "\n",
    "We define a single `BertModel` encoder plus:\n",
    "\n",
    "- **Classification head** (pooled output → 2 classes)  \n",
    "- **NER head** (token outputs → 4 classes)  \n",
    "- Shared dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad761ce4-9e12-47ab-9815-48b1e96fd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3.1 Shared BERT encoder\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # 3.2 Task A: Sentence classification head\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        \n",
    "        # 3.3 Task B: Token-level NER head\n",
    "        self.ner = nn.Linear(self.bert.config.hidden_size, 4)\n",
    "        \n",
    "        # 3.4 Dropout to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 3.5 Pass inputs through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 3.6 Pooled [CLS] output for classification\n",
    "        pooled = outputs.pooler_output\n",
    "        \n",
    "        # 3.7 Full sequence output for token tagging\n",
    "        sequence = outputs.last_hidden_state\n",
    "        \n",
    "        # 3.8 Compute logits for each task\n",
    "        logits_a = self.classifier(self.dropout(pooled))          # shape: (batch, 2)\n",
    "        logits_b = self.ner(self.dropout(sequence))               # shape: (batch, seq_len, 4)\n",
    "        \n",
    "        return logits_a, logits_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c06d5-1c14-45c3-9fdd-b1575ecc0d04",
   "metadata": {},
   "source": [
    "# 4. Training Setup\n",
    "\n",
    "- **Loss functions**:  \n",
    "  - Task A: `CrossEntropyLoss` over sentence logits  \n",
    "  - Task B: `CrossEntropyLoss` over flattened token logits  \n",
    "- **Optimizer**: AdamW with a small learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41d71d4-d6ba-49ab-83f7-a7945f60d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pi/miniconda3/envs/cleanenv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4.1 Instantiate the model\n",
    "model = MultiTaskTransformer()\n",
    "\n",
    "# 4.2 Define losses\n",
    "loss_fn_a = nn.CrossEntropyLoss()\n",
    "loss_fn_b = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4.3 Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # convergence \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63079683-bd37-43e6-8e82-9254ffb3364d",
   "metadata": {},
   "source": [
    "# 5. Collate Function & DataLoader\n",
    "\n",
    "We need to pad NER label lists up to `max_length` so they align with token sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93ffbd4-cabc-4995-96ef-b0f79db1fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch is a list of dicts:\n",
    "      - input_ids: tensor [seq_len]\n",
    "      - attention_mask: tensor [seq_len]\n",
    "      - task_a_labels: int\n",
    "      - task_b_labels: list[int]\n",
    "    We stack input tensors and pad NER labels to length 128.\n",
    "    \"\"\"\n",
    "    input_ids    = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention    = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    task_a       = torch.tensor([item[\"task_a_labels\"] for item in batch])\n",
    "    \n",
    "    # Pad each task_b_labels list to length 128\n",
    "    task_b_padded = [\n",
    "        torch.nn.functional.pad(torch.tensor(lbls), (0, 128 - len(lbls)), value=-100)\n",
    "        for lbls in (item[\"task_b_labels\"] for item in batch)\n",
    "    ]\n",
    "    task_b = torch.stack(task_b_padded)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention,\n",
    "        \"task_a_labels\": task_a,\n",
    "        \"task_b_labels\": task_b\n",
    "    }\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84041848-ffb9-4153-bb91-089bdffc723a",
   "metadata": {},
   "source": [
    "# 6. Inference Example\n",
    "\n",
    "A helper function to run both tasks on a single input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8071c64-6618-4799-b953-bfeaa1646bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative', 'entities': [('christopher', 1), ('nolan', 3), ('directed', 1), ('inception', 2), ('in', 0), ('london', 1)]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(text: str):\n",
    "    # Tokenize single sentence\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_a, logits_b = model(enc.input_ids, enc.attention_mask)\n",
    "    \n",
    "    # Task A: classification label\n",
    "    pred_a = torch.argmax(logits_a, dim=1).item()\n",
    "    sentiment = \"Positive\" if pred_a == 1 else \"Negative\"\n",
    "    \n",
    "    # Task B: NER tags per token\n",
    "    tags = torch.argmax(logits_b, dim=2).squeeze().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc.input_ids[0])\n",
    "    # entities = list(zip(tokens, tags))\n",
    "    filtered_entities = [ (tok, tag) for tok, tag in zip(tokens, tags) if tok not in ['[PAD]', '[CLS]', '[SEP]'] ]\n",
    "    \n",
    "    return {\"sentiment\": sentiment, \"entities\": filtered_entities}\n",
    "\n",
    "# Test\n",
    "sample = \"Christopher Nolan directed Inception in London\"\n",
    "print(predict(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3f33a-c5ba-482f-9914-d048f1a25e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79eeb2c-60fc-47df-8b37-2f82915ca7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313b6dc-859b-46ce-9714-57d2dc01cd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
