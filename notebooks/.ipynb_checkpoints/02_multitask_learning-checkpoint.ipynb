{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ec34a-67e0-476f-83da-630bff5d895e",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion\n",
    "Expand the sentence transformer to handle a multi-task learning setting.\n",
    "1. Task A: Sentence Classification – Classify sentences into predefined classes (you can make these up).\n",
    "2. Task B: [Choose another relevant NLP task such as Named Entity Recognition,\n",
    "Sentiment Analysis, etc.] (you can make the labels up)\n",
    "Describe the changes made to the architecture to support multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962cad1-0531-4e4b-897e-bd9e520634c0",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to extend a sentence-transformer model (BERT) to perform **two** tasks simultaneously:\n",
    "\n",
    "1. **Task A: Sentence Classification**  \n",
    "2. **Task B: Named Entity Recognition (NER)**\n",
    "\n",
    "We share a single encoder and add two task-specific heads. We then train with a combined loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9cfb5-0796-4d8b-bafe-b1a8881d05a8",
   "metadata": {},
   "source": [
    "# 1. Imports Libraries\n",
    "\n",
    "We load PyTorch, HuggingFace Transformers, and the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd726bca-38bd-444b-aaec-f87524c1d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde42689-242b-4792-9e7c-cc2208f1f55c",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "- **Task A**: Sentiment classification on IMDB (binary: positive vs. negative).  \n",
    "- **Task B**: Synthetic NER labels (4 classes: O, PER, LOC, ORG) for demonstration only.\n",
    "\n",
    "We’ll:\n",
    "1. Load the IMDB dataset.\n",
    "2. Tokenize text to fixed length.\n",
    "3. Create randomm NER labels aligned to token count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634310-7598-40f4-bfa4-8026af3f5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load the IMDB dataset (train & test splits)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 2.2 Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_ner_labels(text: str):\n",
    "    \"\"\"\n",
    "    Generate synthetic token-level labels.\n",
    "    In a real setting, you’d use human-annotated NER tags.\n",
    "    Returns a list of random ints (0-3) matching the tokenized length.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return np.random.randint(0, 4, size=(len(tokens),)).tolist()\n",
    "\n",
    "def preprocess(batch):\n",
    "    \"\"\"\n",
    "    Tokenize the batch of texts, create Task A & B labels.\n",
    "    Returns PyTorch tensors and raw label lists.\n",
    "    \"\"\"\n",
    "    # Convert IMDB 0/1 labels to our binary classification labels\n",
    "    task_a_labels = [1 if lbl == 1 else 0 for lbl in batch[\"label\"]]\n",
    "    \n",
    "    # Generate synthetic NER labels per example\n",
    "    task_b_labels = [create_ner_labels(txt) for txt in batch[\"text\"]]\n",
    "\n",
    "    # Tokenize to max_length=128\n",
    "    enc = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"task_a_labels\": task_a_labels,\n",
    "        \"task_b_labels\": task_b_labels\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to the train split\n",
    "dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\",\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b557e-ddc9-405d-a68a-1c7a443bbcf1",
   "metadata": {},
   "source": [
    "# 3. Model Architecture\n",
    "\n",
    "We define a single `BertModel` encoder plus:\n",
    "\n",
    "- **Classification head** (pooled output → 2 classes)  \n",
    "- **NER head** (token outputs → 4 classes)  \n",
    "- Shared dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad761ce4-9e12-47ab-9815-48b1e96fd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3.1 Shared BERT encoder\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # 3.2 Task A: Sentence classification head\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        \n",
    "        # 3.3 Task B: Token-level NER head\n",
    "        self.ner = nn.Linear(self.bert.config.hidden_size, 4)\n",
    "        \n",
    "        # 3.4 Dropout to reduce overfitting\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 3.5 Pass inputs through BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 3.6 Pooled [CLS] output for classification\n",
    "        pooled = outputs.pooler_output\n",
    "        \n",
    "        # 3.7 Full sequence output for token tagging\n",
    "        sequence = outputs.last_hidden_state\n",
    "        \n",
    "        # 3.8 Compute logits for each task\n",
    "        logits_a = self.classifier(self.dropout(pooled))          # shape: (batch, 2)\n",
    "        logits_b = self.ner(self.dropout(sequence))               # shape: (batch, seq_len, 4)\n",
    "        \n",
    "        return logits_a, logits_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c06d5-1c14-45c3-9fdd-b1575ecc0d04",
   "metadata": {},
   "source": [
    "# 4. Training Setup\n",
    "\n",
    "- **Loss functions**:  \n",
    "  - Task A: `CrossEntropyLoss` over sentence logits  \n",
    "  - Task B: `CrossEntropyLoss` over flattened token logits  \n",
    "- **Optimizer**: AdamW with a small learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d71d4-d6ba-49ab-83f7-a7945f60d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.1 Instantiate the model\n",
    "model = MultiTaskTransformer()\n",
    "\n",
    "# 4.2 Define losses\n",
    "loss_fn_a = nn.CrossEntropyLoss()\n",
    "loss_fn_b = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4.3 Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63079683-bd37-43e6-8e82-9254ffb3364d",
   "metadata": {},
   "source": [
    "# 5. Collate Function & DataLoader\n",
    "\n",
    "We need to pad NER label lists up to `max_length` so they align with token sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ffbd4-cabc-4995-96ef-b0f79db1fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Batch is a list of dicts:\n",
    "      - input_ids: tensor [seq_len]\n",
    "      - attention_mask: tensor [seq_len]\n",
    "      - task_a_labels: int\n",
    "      - task_b_labels: list[int]\n",
    "    We stack input tensors and pad NER labels to length 128.\n",
    "    \"\"\"\n",
    "    input_ids    = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention    = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    task_a       = torch.tensor([item[\"task_a_labels\"] for item in batch])\n",
    "    \n",
    "    # Pad each task_b_labels list to length 128\n",
    "    task_b_padded = [\n",
    "        torch.nn.functional.pad(torch.tensor(lbls), (0, 128 - len(lbls)), value=-100)\n",
    "        for lbls in (item[\"task_b_labels\"] for item in batch)\n",
    "    ]\n",
    "    task_b = torch.stack(task_b_padded)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention,\n",
    "        \"task_a_labels\": task_a,\n",
    "        \"task_b_labels\": task_b\n",
    "    }\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b406dd-f585-4c5e-a768-49eb8d2d91c8",
   "metadata": {},
   "source": [
    "# 6. Training Loop\n",
    "\n",
    "For each epoch and batch:\n",
    "1. Forward pass through shared encoder + both heads  \n",
    "2. Compute Task A and Task B losses  \n",
    "3. Sum losses (here equally weighted)  \n",
    "4. Backpropagate & optimizer step  \n",
    "5. Track average loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072093d3-1179-4d72-adc0-43dcedbd4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Batch preparation for multi-task training.\n",
    "    \n",
    "#     Args:\n",
    "#         batch: list of dicts, each containing:\n",
    "#             - \"input_ids\":      list[int] of token IDs\n",
    "#             - \"attention_mask\": list[int] mask (1=real token, 0=padding)\n",
    "#             - \"task_a_labels\":  int label for sentence classification\n",
    "#             - \"task_b_labels\":  list[int] of NER labels (one per token)\n",
    "    \n",
    "#     Returns:\n",
    "#         A dict of batched Tensors:\n",
    "#             input_ids:      LongTensor of shape (batch_size, seq_len)\n",
    "#             attention_mask: LongTensor of shape (batch_size, seq_len)\n",
    "#             task_a_labels:  LongTensor of shape (batch_size,)\n",
    "#             task_b_labels:  LongTensor of shape (batch_size, seq_len)\n",
    "#     \"\"\"\n",
    "#     # 1. Convert input_ids and attention_mask lists into tensors and stack\n",
    "#     #    Resulting shape: (batch_size, seq_len)\n",
    "#     input_ids = torch.stack([\n",
    "#         torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "#         for item in batch\n",
    "#     ])\n",
    "#     attention_mask = torch.stack([\n",
    "#         torch.tensor(item[\"attention_mask\"], dtype=torch.long)\n",
    "#         for item in batch\n",
    "#     ])\n",
    "    \n",
    "#     # 2. Convert sentence-level labels (Task A) into a 1D tensor\n",
    "#     #    Shape: (batch_size,)\n",
    "#     task_a_labels = torch.tensor(\n",
    "#         [item[\"task_a_labels\"] for item in batch],\n",
    "#         dtype=torch.long\n",
    "#     )\n",
    "    \n",
    "#     # 3. Prepare token-level labels (Task B):\n",
    "#     #    - Each example’s label list may be shorter than seq_len.\n",
    "#     #    - Pad to length=128 with value -100, so these positions are ignored by loss_fn.\n",
    "#     #    - Stack into shape: (batch_size, seq_len)\n",
    "#     task_b_labels = torch.stack([\n",
    "#         F.pad(\n",
    "#             torch.tensor(item[\"task_b_labels\"], dtype=torch.long),\n",
    "#             pad=(0, 128 - len(item[\"task_b_labels\"])),  # (left, right) padding\n",
    "#             value=-100                                  # ignore index for CrossEntropyLoss\n",
    "#         )\n",
    "#         for item in batch\n",
    "#     ])\n",
    "    \n",
    "#     # 4. Return the collated batch dictionary\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"task_a_labels\": task_a_labels,\n",
    "#         \"task_b_labels\": task_b_labels\n",
    "#     }\n",
    "\n",
    "# # Recreate the DataLoader to use our custom collate function.\n",
    "# # Shuffle for training, batch_size=16.\n",
    "# train_loader = DataLoader(\n",
    "#     dataset[\"train\"],\n",
    "#     batch_size=16,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84041848-ffb9-4153-bb91-089bdffc723a",
   "metadata": {},
   "source": [
    "# 7. Inference Example\n",
    "\n",
    "A helper function to run both tasks on a single input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8071c64-6618-4799-b953-bfeaa1646bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(text: str):\n",
    "    # Tokenize single sentence\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_a, logits_b = model(enc.input_ids, enc.attention_mask)\n",
    "    \n",
    "    # Task A: classification label\n",
    "    pred_a = torch.argmax(logits_a, dim=1).item()\n",
    "    sentiment = \"Positive\" if pred_a == 1 else \"Negative\"\n",
    "    \n",
    "    # Task B: NER tags per token\n",
    "    tags = torch.argmax(logits_b, dim=2).squeeze().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(enc.input_ids[0])\n",
    "    entities = list(zip(tokens, tags))\n",
    "    \n",
    "    return {\"sentiment\": sentiment, \"entities\": entities}\n",
    "\n",
    "# Test\n",
    "sample = \"Christopher Nolan directed Inception in London\"\n",
    "print(predict(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0414f-2213-4020-b73c-9c3350da7e55",
   "metadata": {},
   "source": [
    "## 8. Summary of Architectural Changes\n",
    "\n",
    "- **Shared Encoder**: One BERT processes all inputs.  \n",
    "- **Task Heads**: Separate linear layers for sentence-level and token-level outputs.  \n",
    "- **Dropout**: Regularizes both heads.  \n",
    "- **Combined Loss**: Sum of classification and NER cross-entropy.  \n",
    "- **Padding Strategy**: NER labels padded to match token length, with `-100` to ignore in the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3f33a-c5ba-482f-9914-d048f1a25e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79eeb2c-60fc-47df-8b37-2f82915ca7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313b6dc-859b-46ce-9714-57d2dc01cd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
